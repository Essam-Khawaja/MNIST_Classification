{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac56124",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "In this project, I am to essentially build the 'Hello World!' of Deep Learning. I hope to apply all the knowledge I have gained in my courses to build my very first deep learning algorithm.\n",
    "\n",
    "The dataset, MNIST, is for Handwritten Digit Recognition. This project aims to classify each handwritten digit to the actual digit. The goal is to build a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3823df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1472e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "# Note that tfds does not have a validation set, we will have to make one on our own.\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)  # This method casts the first parameter to the data type provided in the second parameter\n",
    "\n",
    "# Lets also get easier access to the number of test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1209110",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We know that the MNIST dataset consists of images with each pixel ranging from 0 to 255 in terms of their 'blackness', with 255 meaning white and 0 meaning black. We need to standardize this before we apply any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db54cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)  # Make sure the image is a float\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcc31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale)   # Maps each input to the function return\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821dee5",
   "metadata": {},
   "source": [
    "Now, we also need to shuffle and batch the data so that it is randomly spread, thus giving better accuracy to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662fe4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000 # Take 10,000 samples at a time, shuffle them, then take the next one\n",
    "# This is needed because the dataset is large.\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)  # Shuffle function takes the buffer\n",
    "\n",
    "# Lets also split the training and validation sets\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552b8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 23:04:47.944982: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-07-03 23:04:48.203080: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) # Techinically, this just creates a single batch of the entire thing. \n",
    "# The reason we need this is because the tensor will now have a 'batch' column that it can use, so it will not get confused when trying to forward propogate later on\n",
    "scaled_test_data = scaled_test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) # The next() function loads the next element of the iterable object, in this case our batches\n",
    "# Since there is one batch, it will just load the inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f80561",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now that we have preprocessed the data by:\n",
    "1. Scaling all the pixel inputs from 0 to 1 by dividing it by 255.\n",
    "2. Made sure that the training, validation and test sets are all batched.\n",
    "\n",
    "We can now move on to actually building the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7c95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_size = 784 # Total number of pixels in the image (our image is 28x28)\n",
    "output_size = 10    # There are 10 digits to choose from (or classify into)\n",
    "hidden_layer_size = 50  # The assumption is that all hidden layers are the same size\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # First of all, our inputs are of a tensor size 28x28x1, which we cannot work with\n",
    "    # To resolve this, we need to flatten this tensor into a single vector of size 784\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # This method takes the dot product of the inputs and the weights then adds the bias\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),    # This is us stacking the layer\n",
    "\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),    # Our final layer (output)\n",
    "    # Softmax gives us the probability\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c16443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# The sparse loss function one-hot encodes our data\n",
    "# The metrics is for what you need to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0339ca",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f441fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 1s - 3ms/step - accuracy: 0.8839 - loss: 0.4134 - val_accuracy: 0.9377 - val_loss: 0.2194\n",
      "Epoch 2/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9464 - loss: 0.1848 - val_accuracy: 0.9503 - val_loss: 0.1674\n",
      "Epoch 3/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9588 - loss: 0.1397 - val_accuracy: 0.9613 - val_loss: 0.1262\n",
      "Epoch 4/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9662 - loss: 0.1158 - val_accuracy: 0.9655 - val_loss: 0.1132\n",
      "Epoch 5/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9714 - loss: 0.0966 - val_accuracy: 0.9713 - val_loss: 0.0973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x345b95910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCH_NUM = 5\n",
    "\n",
    "model.fit(train_data, epochs=EPOCH_NUM, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da774b",
   "metadata": {},
   "source": [
    "As we can see, our model has a 97% accuracy! This is decent, but we can improve the model further by increasing the number of hidden layers and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368a4c0",
   "metadata": {},
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa7723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 - 1s - 2ms/step - accuracy: 0.9713 - loss: 0.1219 - val_accuracy: 0.9702 - val_loss: 0.1542\n",
      "Epoch 2/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9751 - loss: 0.1034 - val_accuracy: 0.9730 - val_loss: 0.0993\n",
      "Epoch 3/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9746 - loss: 0.1048 - val_accuracy: 0.9695 - val_loss: 0.1172\n",
      "Epoch 4/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9743 - loss: 0.1106 - val_accuracy: 0.9680 - val_loss: 0.1344\n",
      "Epoch 5/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9747 - loss: 0.1096 - val_accuracy: 0.9717 - val_loss: 0.1147\n",
      "Epoch 6/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9761 - loss: 0.1001 - val_accuracy: 0.9775 - val_loss: 0.0914\n",
      "Epoch 7/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9756 - loss: 0.1054 - val_accuracy: 0.9703 - val_loss: 0.1326\n",
      "Epoch 8/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9752 - loss: 0.1106 - val_accuracy: 0.9690 - val_loss: 0.1173\n",
      "Epoch 9/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9766 - loss: 0.0950 - val_accuracy: 0.9742 - val_loss: 0.1048\n",
      "Epoch 10/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9779 - loss: 0.0937 - val_accuracy: 0.9702 - val_loss: 0.1279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x34b8f0da0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_size = 200\n",
    "\n",
    "improved_model = tf.keras.Sequential([\n",
    "    # First of all, our inputs are of a tensor size 28x28x1, which we cannot work with\n",
    "    # To resolve this, we need to flatten this tensor into a single vector of size 784\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # This method takes the dot product of the inputs and the weights then adds the bias\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),    # This is us stacking the layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),    # Our final layer (output)\n",
    "    # Softmax gives us the probability\n",
    "])\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "improved_model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCH_NUM = 10\n",
    "model.fit(train_data, epochs=EPOCH_NUM, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf201c",
   "metadata": {},
   "source": [
    "I do not think I can get a better accuracy than this without having to restart the entire model. Looking at the tensorflow documentation, it seems that 97% is already a pretty good accuracy, as the solution they have provided only takes us to 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63b41e",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d1a384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9580 - loss: 0.2754\n",
      "0.2754209041595459 0.9580000042915344\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(scaled_test_data)\n",
    "print(test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502e657",
   "metadata": {},
   "source": [
    "Ok, so we actually have a 95.8% test accuracy, meaning we have slightly overfit the hyperparameters of the model to the validation set during training. But still, a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d30e7d",
   "metadata": {},
   "source": [
    "And with that, this concludes me playing around with the MNIST dataset! Thanks for reading through me bumbling around with all this new syntax and theory. With such deep possibilites, I think I will most likely play around more with deep neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
