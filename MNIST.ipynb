{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac56124",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "In this project, I am to essentially build the 'Hello World!' of Deep Learning. I hope to apply all the knowledge I have gained in my courses to build my very first deep learning algorithm.\n",
    "\n",
    "The dataset, MNIST, is for Handwritten Digit Recognition. This project aims to classify each handwritten digit to the actual digit. The goal is to build a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3823df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1472e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "# Note that tfds does not have a validation set, we will have to make one on our own.\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)  # This method casts the first parameter to the data type provided in the second parameter\n",
    "\n",
    "# Lets also get easier access to the number of test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1209110",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We know that the MNIST dataset consists of images with each pixel ranging from 0 to 255 in terms of their 'blackness', with 255 meaning white and 0 meaning black. We need to standardize this before we apply any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db54cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)  # Make sure the image is a float\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcc31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale)   # Maps each input to the function return\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821dee5",
   "metadata": {},
   "source": [
    "Now, we also need to shuffle and batch the data so that it is randomly spread, thus giving better accuracy to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662fe4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000 # Take 10,000 samples at a time, shuffle them, then take the next one\n",
    "# This is needed because the dataset is large.\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)  # Shuffle function takes the buffer\n",
    "\n",
    "# Lets also split the training and validation sets\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552b8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 22:49:14.192096: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-07-03 22:49:14.456655: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) # Techinically, this just creates a single batch of the entire thing. \n",
    "# The reason we need this is because the tensor will now have a 'batch' column that it can use, so it will not get confused when trying to forward propogate later on\n",
    "scaled_test_data = scaled_test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) # The next() function loads the next element of the iterable object, in this case our batches\n",
    "# Since there is one batch, it will just load the inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f80561",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now that we have preprocessed the data by:\n",
    "1. Scaling all the pixel inputs from 0 to 1 by dividing it by 255.\n",
    "2. Made sure that the training, validation and test sets are all batched.\n",
    "\n",
    "We can now move on to actually building the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7c95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_size = 784 # Total number of pixels in the image (our image is 28x28)\n",
    "output_size = 10    # There are 10 digits to choose from (or classify into)\n",
    "hidden_layer_size = 50  # The assumption is that all hidden layers are the same size\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # First of all, our inputs are of a tensor size 28x28x1, which we cannot work with\n",
    "    # To resolve this, we need to flatten this tensor into a single vector of size 784\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # This method takes the dot product of the inputs and the weights then adds the bias\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),    # This is us stacking the layer\n",
    "\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),    # Our final layer (output)\n",
    "    # Softmax gives us the probability\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c16443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# The sparse loss function one-hot encodes our data\n",
    "# The metrics is for what you need to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0339ca",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f441fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9762 - loss: 0.0778 - val_accuracy: 0.9785 - val_loss: 0.0778\n",
      "Epoch 2/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9795 - loss: 0.0704 - val_accuracy: 0.9767 - val_loss: 0.0744\n",
      "Epoch 3/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9818 - loss: 0.0613 - val_accuracy: 0.9748 - val_loss: 0.0761\n",
      "Epoch 4/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9836 - loss: 0.0553 - val_accuracy: 0.9803 - val_loss: 0.0642\n",
      "Epoch 5/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9847 - loss: 0.0506 - val_accuracy: 0.9808 - val_loss: 0.0667\n",
      "Epoch 6/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9859 - loss: 0.0465 - val_accuracy: 0.9832 - val_loss: 0.0562\n",
      "Epoch 7/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9871 - loss: 0.0424 - val_accuracy: 0.9858 - val_loss: 0.0501\n",
      "Epoch 8/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9884 - loss: 0.0374 - val_accuracy: 0.9885 - val_loss: 0.0448\n",
      "Epoch 9/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9903 - loss: 0.0329 - val_accuracy: 0.9868 - val_loss: 0.0457\n",
      "Epoch 10/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9900 - loss: 0.0306 - val_accuracy: 0.9875 - val_loss: 0.0420\n",
      "Epoch 11/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9914 - loss: 0.0288 - val_accuracy: 0.9877 - val_loss: 0.0402\n",
      "Epoch 12/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9916 - loss: 0.0273 - val_accuracy: 0.9875 - val_loss: 0.0382\n",
      "Epoch 13/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9933 - loss: 0.0225 - val_accuracy: 0.9907 - val_loss: 0.0297\n",
      "Epoch 14/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9936 - loss: 0.0212 - val_accuracy: 0.9923 - val_loss: 0.0266\n",
      "Epoch 15/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9931 - loss: 0.0215 - val_accuracy: 0.9895 - val_loss: 0.0311\n",
      "Epoch 16/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9942 - loss: 0.0191 - val_accuracy: 0.9922 - val_loss: 0.0246\n",
      "Epoch 17/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9947 - loss: 0.0174 - val_accuracy: 0.9933 - val_loss: 0.0196\n",
      "Epoch 18/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9954 - loss: 0.0148 - val_accuracy: 0.9940 - val_loss: 0.0189\n",
      "Epoch 19/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9935 - loss: 0.0189 - val_accuracy: 0.9915 - val_loss: 0.0249\n",
      "Epoch 20/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9958 - loss: 0.0138 - val_accuracy: 0.9898 - val_loss: 0.0283\n",
      "Epoch 21/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9962 - loss: 0.0126 - val_accuracy: 0.9950 - val_loss: 0.0159\n",
      "Epoch 22/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9962 - loss: 0.0121 - val_accuracy: 0.9925 - val_loss: 0.0179\n",
      "Epoch 23/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9959 - loss: 0.0128 - val_accuracy: 0.9950 - val_loss: 0.0159\n",
      "Epoch 24/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9969 - loss: 0.0102 - val_accuracy: 0.9927 - val_loss: 0.0218\n",
      "Epoch 25/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9968 - loss: 0.0106 - val_accuracy: 0.9952 - val_loss: 0.0129\n",
      "Epoch 26/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9967 - loss: 0.0108 - val_accuracy: 0.9965 - val_loss: 0.0108\n",
      "Epoch 27/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9964 - loss: 0.0113 - val_accuracy: 0.9958 - val_loss: 0.0117\n",
      "Epoch 28/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9966 - loss: 0.0103 - val_accuracy: 0.9970 - val_loss: 0.0097\n",
      "Epoch 29/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9977 - loss: 0.0078 - val_accuracy: 0.9920 - val_loss: 0.0214\n",
      "Epoch 30/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 0.9975 - val_loss: 0.0078\n",
      "Epoch 31/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 0.9992 - val_loss: 0.0053\n",
      "Epoch 32/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 0.9963 - val_loss: 0.0126\n",
      "Epoch 33/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9964 - loss: 0.0110 - val_accuracy: 0.9933 - val_loss: 0.0196\n",
      "Epoch 34/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9966 - loss: 0.0102 - val_accuracy: 0.9953 - val_loss: 0.0149\n",
      "Epoch 35/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9971 - loss: 0.0085 - val_accuracy: 0.9973 - val_loss: 0.0081\n",
      "Epoch 36/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9977 - loss: 0.0068 - val_accuracy: 0.9957 - val_loss: 0.0151\n",
      "Epoch 37/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.9975 - val_loss: 0.0077\n",
      "Epoch 38/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9977 - loss: 0.0077 - val_accuracy: 0.9970 - val_loss: 0.0080\n",
      "Epoch 39/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9969 - loss: 0.0086 - val_accuracy: 0.9963 - val_loss: 0.0100\n",
      "Epoch 40/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9970 - loss: 0.0091 - val_accuracy: 0.9980 - val_loss: 0.0075\n",
      "Epoch 41/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 0.9957 - val_loss: 0.0118\n",
      "Epoch 42/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9988 - loss: 0.0035 - val_accuracy: 0.9988 - val_loss: 0.0037\n",
      "Epoch 43/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.9990 - val_loss: 0.0044\n",
      "Epoch 44/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 0.9990 - val_loss: 0.0044\n",
      "Epoch 45/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 0.9937 - val_loss: 0.0187\n",
      "Epoch 46/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9972 - loss: 0.0089 - val_accuracy: 0.9980 - val_loss: 0.0069\n",
      "Epoch 47/100\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 0.9982 - val_loss: 0.0070\n",
      "Epoch 48/100\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 5\n",
    "\n",
    "model.fit(train_data, epochs=EPOCH_NUM, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da774b",
   "metadata": {},
   "source": [
    "This model, at first, has 97% accuracy. I will now change the epoch number from 5 to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75eb2c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
